{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_46pUC5x6NE"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install all dependencies\n",
        "# Colab cell: replicate your Space conversion (install deps, clone transformers.js, run scripts.convert)\n",
        "\n",
        "!pip install -q \\\n",
        "    huggingface_hub \\\n",
        "    streamlit \\\n",
        "    optimum==1.27.0 \\\n",
        "    onnx==1.16.0 \\\n",
        "    tqdm==4.67.1 \\\n",
        "    onnxslim==0.1.48 \\\n",
        "    numpy==2.2.6 \\\n",
        "    sacremoses==0.0.53 \\\n",
        "    sentencepiece==0.1.99 \\\n",
        "    onnxruntime-gpu==1.20.0 \\\n",
        "    onnxruntime-tools==1.7.0 \\\n",
        "    \"transformers[torch]==4.49.0\"\n",
        "\n",
        "import os, shutil, subprocess, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Clean up any old repo copy\n",
        "repo_dir = Path(\"transformers.js\")\n",
        "if repo_dir.exists():\n",
        "    shutil.rmtree(repo_dir)\n",
        "\n",
        "# Clone transformers.js repo (default branch is fine if tag not available)\n",
        "!git clone --depth 1 https://github.com/huggingface/transformers.js.git\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Prompt: confirm installed libraries and their versions\n",
        "# -------------------------------------------------\n",
        "import importlib\n",
        "\n",
        "def pkg_version(pkg_name):\n",
        "    try:\n",
        "        return importlib.import_module(pkg_name).__version__\n",
        "    except Exception:\n",
        "        return \"not found\"\n",
        "\n",
        "libs = {\n",
        "    \"huggingface_hub\": \"huggingface_hub\",\n",
        "    \"streamlit\": \"streamlit\",\n",
        "    \"transformers\": \"transformers\",\n",
        "    \"optimum\": \"optimum\",\n",
        "    \"onnx\": \"onnx\",\n",
        "    \"tqdm\": \"tqdm\",\n",
        "    \"onnxslim\": \"onnxslim\",\n",
        "    \"numpy\": \"numpy\",\n",
        "    \"sacremoses\": \"sacremoses\",\n",
        "    \"sentencepiece\": \"sentencepiece\",\n",
        "    \"onnxruntime\": \"onnxruntime\",\n",
        "    \"onnxruntime_gpu\": \"onnxruntime_gpu\",\n",
        "    \"onnxruntime_tools\": \"onnxruntime_tools\",\n",
        "}\n",
        "\n",
        "print(\"\\n‚úÖ Installed library versions:\")\n",
        "for display_name, import_name in libs.items():\n",
        "    print(f\"  {display_name}: {pkg_version(import_name)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Convert a Hugging Face model to ONNX (without quantization)\n",
        "\n",
        "import os, sys, subprocess\n",
        "\n",
        "repo_dir = \"transformers.js\"\n",
        "env = os.environ.copy()\n",
        "env[\"PYTHONWARNINGS\"] = \"ignore::torch.jit.TracerWarning\"\n",
        "\n",
        "# üîπ Change this to any model you want to convert\n",
        "model_id = \"USERNAME/MODEL_ID\"\n",
        "\n",
        "convert_cmd = [\n",
        "    sys.executable, \"-m\", \"scripts.convert\",\n",
        "    \"--model_id\", model_id,\n",
        "    #\"--opset\", \"13\"          # ‚Üê add this line, this opset will produce ONNX files with IR‚ÄØ8\n",
        "    # (no --quantize)\n",
        "]\n",
        "\n",
        "\n",
        "print(f\"\\nRunning ONNX conversion for: {model_id}\")\n",
        "res = subprocess.run(\n",
        "    convert_cmd,\n",
        "    cwd=repo_dir,\n",
        "    env=env,\n",
        "    text=True,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT\n",
        ")\n",
        "\n",
        "print(\"Conversion return code:\", res.returncode)\n",
        "with open(\"transform_conversion.log\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(res.stdout)\n",
        "\n",
        "print(\"\\n---- Last 2000 chars of conversion log ----\")\n",
        "print(res.stdout[-2000:])\n",
        "\n",
        "if res.returncode == 0:\n",
        "    print(f\"\\n‚úÖ Conversion successful! Check {repo_dir}/models/{model_id}/\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Conversion failed. Check transform_conversion.log for details.\")\n"
      ],
      "metadata": {
        "id": "MVeC7-aKybME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell‚ÄØX: Verify the exported ONNX model (IR version, opset, etc.)\n",
        "\n",
        "# ---- Path to the exported ONNX file(s)  ----\n",
        "# Adjust the model name if you exported a different checkpoint\n",
        "import pathlib\n",
        "import onnx\n",
        "\n",
        "model_dir   = pathlib.Path(\"transformers.js/models/USERNAME/MODEL_ID/onnx\")\n",
        "encoder_path = model_dir / \"encoder_model.onnx\"\n",
        "decoder_path = model_dir / \"decoder_model.onnx\"   # or *_merged.onnx, etc.\n",
        "\n",
        "def check_onnx(path):\n",
        "    print(f\"\\nüîé Checking: {path}\")\n",
        "    model = onnx.load(str(path))\n",
        "\n",
        "    # IR version (must be ‚â§‚ÄØ8 for onnxruntime‚Äëweb 1.19.x)\n",
        "    print(\" ‚Ä¢ ir_version :\", model.ir_version)\n",
        "\n",
        "    # Opset versions (read from the opset_import list)\n",
        "    opset_versions = {imp.domain: imp.version for imp in model.opset_import}\n",
        "    print(\" ‚Ä¢ opset_import :\", opset_versions)\n",
        "\n",
        "    # Graph statistics\n",
        "    print(f\" ‚Ä¢ graph: {len(model.graph.node)} nodes, \"\n",
        "          f\"{len(model.graph.input)} inputs, \"\n",
        "          f\"{len(model.graph.output)} outputs\")\n",
        "\n",
        "    # Optional: quick shape check (first input)\n",
        "    if model.graph.input:\n",
        "        shape = [dim.dim_value for dim in model.graph.input[0].type.tensor_type.shape.dim]\n",
        "        print(\" ‚Ä¢ first input shape :\", shape)\n",
        "\n",
        "# Run the checks\n",
        "check_onnx(encoder_path)\n",
        "check_onnx(decoder_path)"
      ],
      "metadata": {
        "id": "IfJ6MsGQ4Q3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Quantize ONNX files in-place (save into the same onnx/ folder)\n",
        "# -------------------------------------------------------------\n",
        "# Imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import (\n",
        "    quantize_dynamic,\n",
        "    QuantType,\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Folder that contains the exported ONNX files\n",
        "local_model_dir = Path(\"transformers.js/models/USERNAME/MODEL_ID/onnx\")\n",
        "if not local_model_dir.exists():\n",
        "    raise SystemExit(f\"‚ùå onnx/ folder not found: {local_model_dir}\")\n",
        "\n",
        "onnx_files = list(local_model_dir.glob(\"*.onnx\"))\n",
        "if not onnx_files:\n",
        "    raise SystemExit(f\"‚ùå No .onnx files found in {local_model_dir}\")\n",
        "\n",
        "print(f\"\\nüîé {len(onnx_files)} ONNX file(s) to quantise:\\n\")\n",
        "for f in onnx_files:\n",
        "    print(\" ‚Ä¢\", f.name)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Define the flavours we want (suffix ‚Üí quantisation call)\n",
        "def quantise_file(src: Path):\n",
        "    \"\"\"Create every required quantised variant for a single src file.\"\"\"\n",
        "\n",
        "    # ---- dynamic INT8 (the classic ‚Äú*_quantized.onnx‚Äù) ----\n",
        "    out = src.with_name(src.stem + \"_quantized.onnx\")\n",
        "    quantize_dynamic(str(src), str(out), weight_type=QuantType.QInt8)\n",
        "    print(\"   ‚Üí\", out.name)\n",
        "\n",
        "    # ---- static INT8 (reuse dynamic for demo) ----\n",
        "    out_int8 = src.with_name(src.stem + \"_int8.onnx\")\n",
        "    quantize_dynamic(str(src), str(out_int8), weight_type=QuantType.QInt8)\n",
        "    print(\"   ‚Üí\", out_int8.name)\n",
        "\n",
        "    # ---- UINT8 (dynamic) ----\n",
        "    out_uint8 = src.with_name(src.stem + \"_uint8.onnx\")\n",
        "    quantize_dynamic(str(src), str(out_uint8), weight_type=QuantType.QUInt8)\n",
        "    print(\"   ‚Üí\", out_uint8.name)\n",
        "\n",
        "    # ---- Q4 (dynamic) ----\n",
        "    out_q4 = src.with_name(src.stem + \"_q4.onnx\")\n",
        "    quantize_dynamic(str(src), str(out_q4), weight_type=QuantType.QInt4)\n",
        "    print(\"   ‚Üí\", out_q4.name)\n",
        "\n",
        "    # ---- Q4‚ÄëF16 (dynamic) ‚Äì we keep the Q4 file; FP16 conversion is omitted\n",
        "    out_q4f16 = src.with_name(src.stem + \"_q4f16.onnx\")\n",
        "    quantize_dynamic(str(src), str(out_q4f16), weight_type=QuantType.QInt4)\n",
        "    # (no convert_float_to_float16 call)\n",
        "    print(\"   ‚Üí\", out_q4f16.name)\n",
        "\n",
        "    # ---- FP16 (pure float‚Äë16 conversion) ‚Äì omitted because the helper is unavailable\n",
        "    # If you really need FP16, you can run `onnxruntime-tools` from the command line:\n",
        "    #   !python -m onnxruntime.tools.convert_float_to_float16 <src> <dst>\n",
        "\n",
        "    # ---- BNB4 (copy Q4 file) ----\n",
        "    out_bnb4 = src.with_name(src.stem + \"_bnb4.onnx\")\n",
        "    out_bnb4.write_bytes(out_q4.read_bytes())\n",
        "    print(\"   ‚Üí\", out_bnb4.name)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Run the quantisation for every original file\n",
        "failed = []\n",
        "for src in onnx_files:\n",
        "    try:\n",
        "        print(f\"\\n‚öôÔ∏è  Quantising {src.name} ‚Ä¶\")\n",
        "        quantise_file(src)\n",
        "\n",
        "        # sanity‚Äëcheck: can the runtime load one of the outputs?\n",
        "        sess = ort.InferenceSession(\n",
        "            str(src.with_name(src.stem + \"_quantized.onnx\")),\n",
        "            providers=[\"CPUExecutionProvider\"]\n",
        "        )\n",
        "        print(\"   ‚úÖ  Session created OK\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå  ERROR on {src.name}: {e}\")\n",
        "        failed.append((src, str(e)))\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Summary\n",
        "if failed:\n",
        "    print(\"\\n‚ùó Some files failed to quantise:\")\n",
        "    for src, err in failed:\n",
        "        print(f\" - {src.name}: {err}\")\n",
        "else:\n",
        "    print(\"\\nüéâ  All quantisation variants created successfully in:\", local_model_dir)"
      ],
      "metadata": {
        "id": "944kCpZm4YZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Push converted ONNX model to Hugging Face Hub\n",
        "\n",
        "import os\n",
        "from huggingface_hub import HfApi, HfFolder, upload_folder\n",
        "from getpass import getpass\n",
        "from pathlib import Path\n",
        "\n",
        "# Ask for HF token (write access)\n",
        "if not os.environ.get(\"HF_TOKEN\"):\n",
        "    hf_token = getpass(\"Enter your Hugging Face write token: \")\n",
        "    os.environ[\"HF_TOKEN\"] = hf_token\n",
        "else:\n",
        "    hf_token = os.environ[\"HF_TOKEN\"]\n",
        "\n",
        "# Set model path (from conversion step)\n",
        "model_dir = Path(\"transformers.js/models/USERNAME/MODEL_ID\")\n",
        "\n",
        "# New repo name with -ONNX suffix\n",
        "repo_id = \"NEW_USERNAME/MODEL_ID-ONNX\"\n",
        "\n",
        "# Create the repo if it doesn't exist\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=repo_id, token=hf_token, private=False, exist_ok=True)\n",
        "\n",
        "print(f\"Uploading {model_dir} to {repo_id} ...\")\n",
        "upload_folder(\n",
        "    folder_path=str(model_dir),\n",
        "    repo_id=repo_id,\n",
        "    token=hf_token,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Upload complete! Check your model at: https://huggingface.co/{repo_id}\")\n"
      ],
      "metadata": {
        "id": "20lIv27S5dkL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}